{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Module 1: Symptom Severity Classification using PhoBERT\n",
    "\n",
    "## Objective\n",
    "Develop a classification model to predict **eye symptom severity** across four levels:\n",
    "\n",
    "- **None** ‚Äì No noticeable symptoms\n",
    "- **Mild** ‚Äì Minor discomfort or early symptoms\n",
    "- **Moderate** ‚Äì Clear and persistent symptoms\n",
    "- **Severe** ‚Äì Significant or serious eye strain symptoms\n",
    "\n",
    "## Methodology\n",
    "- Use **PhoBERT (vinai/phobert-base-v2)**, a Vietnamese pre-trained BERT-based language model.\n",
    "- Fine-tune PhoBERT for **multi-class classification**.\n",
    "- Evaluate using multiple metrics:\n",
    "  - **Accuracy**\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-score**\n",
    "  - **Confusion Matrix**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T09:55:39.448998Z",
     "start_time": "2026-02-13T09:55:13.199057Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'vinai/phobert-base-v2',\n",
    "    'num_classes': 4,\n",
    "    'max_length': 128,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 5,\n",
    "    'dropout': 0.3,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'seed': 42,\n",
    "    'save_dir': 'models/module1'\n",
    "}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nüñ•Ô∏è  Using device: {device}\")\n",
    "\n",
    "# Label mapping\n",
    "LABEL_MAP = {\n",
    "    0: \"None\",\n",
    "    1: \"Nh·∫π\",\n",
    "    2: \"V·ª´a\",\n",
    "    3: \"N·∫∑ng\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeSymptomDataset(Dataset):\n",
    "    \"\"\"Dataset for eye symptom severity classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBERTClassifier(nn.Module):\n",
    "    \"\"\"PhoBERT-based symptom classifier with classification head\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, num_classes: int = 4, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classification head with intermediate layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_probs=None):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'precision_weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    for i in range(len(precision_per_class)):\n",
    "        metrics[f'precision_class_{i}'] = precision_per_class[i]\n",
    "        metrics[f'recall_class_{i}'] = recall_per_class[i]\n",
    "        metrics[f'f1_class_{i}'] = f1_per_class[i]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion Matrix'):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def plot_normalized_confusion_matrix(y_true, y_pred, labels, title='Normalized Confusion Matrix'):\n",
    "    \"\"\"Plot normalized confusion matrix (percentage)\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='YlOrRd',\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Percentage'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def plot_metrics_per_class(metrics, labels):\n",
    "    \"\"\"Plot precision, recall, F1 per class\"\"\"\n",
    "    num_classes = len(labels)\n",
    "    \n",
    "    precision = [metrics[f'precision_class_{i}'] for i in range(num_classes)]\n",
    "    recall = [metrics[f'recall_class_{i}'] for i in range(num_classes)]\n",
    "    f1 = [metrics[f'f1_class_{i}'] for i in range(num_classes)]\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "    ax.bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "    ax.bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Metrics per Class', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss over Epochs')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history['val_accuracy'], label='Validation Accuracy', \n",
    "                    marker='o', color='green')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Validation Accuracy over Epochs')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 0].plot(history['val_f1'], label='Validation F1', \n",
    "                    marker='o', color='red')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].set_title('Validation F1 Score over Epochs')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # All metrics combined\n",
    "    axes[1, 1].plot(history['val_accuracy'], label='Accuracy', marker='o')\n",
    "    axes[1, 1].plot(history['val_f1'], label='F1 Score', marker='s')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Validation Metrics Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1Trainer:\n",
    "    \"\"\"Training pipeline for Module 1\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        device: str = 'cpu',\n",
    "        learning_rate: float = 2e-5,\n",
    "        num_epochs: int = 10\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=int(0.1 * total_steps),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Track best model\n",
    "        self.best_val_f1 = 0.0\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'val_f1': [],\n",
    "            'val_precision': [],\n",
    "            'val_recall': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self) -> float:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['label'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(input_ids, attention_mask)\n",
    "            loss = self.criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def evaluate(self, data_loader: DataLoader) -> Dict:\n",
    "        \"\"\"Evaluate model with comprehensive metrics\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                logits = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics = calculate_metrics(all_labels, all_preds, all_probs)\n",
    "        metrics['loss'] = total_loss / len(data_loader)\n",
    "        metrics['predictions'] = all_preds\n",
    "        metrics['labels'] = all_labels\n",
    "        metrics['probabilities'] = all_probs\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def train(self, save_dir: str = \"models/module1\"):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training for {self.num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"\\nüìç Epoch {epoch + 1}/{self.num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_results = self.evaluate(self.val_loader)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_results['loss'])\n",
    "            self.history['val_accuracy'].append(val_results['accuracy'])\n",
    "            self.history['val_f1'].append(val_results['f1_weighted'])\n",
    "            self.history['val_precision'].append(val_results['precision_weighted'])\n",
    "            self.history['val_recall'].append(val_results['recall_weighted'])\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nüìä Metrics:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_results['loss']:.4f}\")\n",
    "            print(f\"  Val Accuracy: {val_results['accuracy']:.4f}\")\n",
    "            print(f\"  Val Precision (weighted): {val_results['precision_weighted']:.4f}\")\n",
    "            print(f\"  Val Recall (weighted): {val_results['recall_weighted']:.4f}\")\n",
    "            print(f\"  Val F1 (weighted): {val_results['f1_weighted']:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_results['f1_weighted'] > self.best_val_f1:\n",
    "                self.best_val_f1 = val_results['f1_weighted']\n",
    "                self.save_model(save_dir, epoch)\n",
    "                print(f\"\\n‚úÖ New best model saved! F1: {self.best_val_f1:.4f}\")\n",
    "        \n",
    "        # Save training history\n",
    "        with open(f\"{save_dir}/training_history.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.history, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nüéâ Training completed! Best Val F1: {self.best_val_f1:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def save_model(self, save_dir: str, epoch: int):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_val_f1': self.best_val_f1,\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, f\"{save_dir}/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Explore Data\n",
    "\n",
    "C√°ch thu th·∫≠p data https://forms.gle/APoRvHVKu9yAXzaC9\n",
    "\n",
    "Full data: 1716\n",
    "\n",
    "Train: 1200\n",
    "\n",
    "Val: 516 (c√°i d∆∞·ªõi ch∆∞a reset √°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data Statistics:\n",
      "  Training samples: 1200\n",
      "  Validation samples: 172\n",
      "\n",
      "üìã Training data preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>severity</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt</td>\n",
       "      <td>1</td>\n",
       "      <td>Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt after long sc...</td>\n",
       "      <td>1</td>\n",
       "      <td>Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt after long sc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M·∫Øt m·ªèi r√µ r·ªát, ph·∫£i nheo khi nh√¨n</td>\n",
       "      <td>3</td>\n",
       "      <td>M·∫Øt m·ªèi r√µ r·ªát, ph·∫£i nheo khi nh√¨n</td>\n",
       "      <td>V·ª´a</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt</td>\n",
       "      <td>1</td>\n",
       "      <td>Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Severe eye strain, hard to keep eyes open t·ª´ s...</td>\n",
       "      <td>4</td>\n",
       "      <td>Severe eye strain, hard to keep eyes open t·ª´ s...</td>\n",
       "      <td>N·∫∑ng</td>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                   Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt      1   \n",
       "1  Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt after long sc...      1   \n",
       "2                 M·∫Øt m·ªèi r√µ r·ªát, ph·∫£i nheo khi nh√¨n      3   \n",
       "3                   Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt      1   \n",
       "4  Severe eye strain, hard to keep eyes open t·ª´ s...      4   \n",
       "\n",
       "                                        text_cleaned severity  text_length  \\\n",
       "0                   Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt      NaN           32   \n",
       "1  Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt after long sc...      NaN           55   \n",
       "2                 M·∫Øt m·ªèi r√µ r·ªát, ph·∫£i nheo khi nh√¨n      V·ª´a           34   \n",
       "3                   Kh√¥ng c√≥ d·∫•u hi·ªáu kh√¥ hay ƒë·ªè m·∫Øt      NaN           32   \n",
       "4  Severe eye strain, hard to keep eyes open t·ª´ s...     N·∫∑ng           57   \n",
       "\n",
       "   word_count  \n",
       "0           8  \n",
       "1          12  \n",
       "2           8  \n",
       "3           8  \n",
       "4          12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(r\"D:\\School\\NLP\\train.csv\")\n",
    "val_df = pd.read_csv(r\"D:\\School\\NLP\\val.csv\")\n",
    "\n",
    "print(f\"üìä Data Statistics:\")\n",
    "print(f\"  Training samples: {len(train_df)}\")\n",
    "print(f\"  Validation samples: {len(val_df)}\")\n",
    "print(f\"\\nüìã Training data preview:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "train_counts = train_df['label'].value_counts().sort_index()\n",
    "axes[0].bar(range(len(train_counts)), train_counts.values, alpha=0.7)\n",
    "axes[0].set_xlabel('Severity Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Training Set - Class Distribution')\n",
    "axes[0].set_xticks(range(len(train_counts)))\n",
    "axes[0].set_xticklabels([LABEL_MAP[i] for i in range(CONFIG['num_classes'])])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "val_counts = val_df['label'].value_counts().sort_index()\n",
    "axes[1].bar(range(len(val_counts)), val_counts.values, alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Severity Level')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Validation Set - Class Distribution')\n",
    "axes[1].set_xticks(range(len(val_counts)))\n",
    "axes[1].set_xticklabels([LABEL_MAP[i] for i in range(CONFIG['num_classes'])])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Class distribution:\")\n",
    "print(\"\\nTraining set:\")\n",
    "for idx, count in train_counts.items():\n",
    "    print(f\"  {LABEL_MAP[idx-1]}: {count} ({count/len(train_df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "for idx, count in val_counts.items():\n",
    "    print(f\"  {LABEL_MAP[idx-1]}: {count} ({count/len(val_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(f\"Loading tokenizer: {CONFIG['model_name']}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "# Create datasets (convert labels from 1-4 to 0-3)\n",
    "train_dataset = EyeSymptomDataset(\n",
    "    train_df['text_cleaned'].tolist(),\n",
    "    (train_df['label'] - 1).tolist(),  # Convert 1-4 to 0-3\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length']\n",
    ")\n",
    "\n",
    "val_dataset = EyeSymptomDataset(\n",
    "    val_df['text_cleaned'].tolist(),\n",
    "    (val_df['label'] - 1).tolist(),\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation completed!\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(f\"Initializing model: {CONFIG['model_name']}\")\n",
    "model = PhoBERTClassifier(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nüèóÔ∏è  Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Module1Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    num_epochs=CONFIG['num_epochs']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = trainer.train(save_dir=CONFIG['save_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig = plot_training_history(history)\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(f\"{CONFIG['save_dir']}/training_history.png\", dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nüíæ Training history plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"\\nüîç Evaluating on validation set...\")\n",
    "val_results = trainer.evaluate(val_loader)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy: {val_results['accuracy']:.4f}\")\n",
    "print(f\"  Precision (macro): {val_results['precision_macro']:.4f}\")\n",
    "print(f\"  Precision (weighted): {val_results['precision_weighted']:.4f}\")\n",
    "print(f\"  Recall (macro): {val_results['recall_macro']:.4f}\")\n",
    "print(f\"  Recall (weighted): {val_results['recall_weighted']:.4f}\")\n",
    "print(f\"  F1-Score (macro): {val_results['f1_macro']:.4f}\")\n",
    "print(f\"  F1-Score (weighted): {val_results['f1_weighted']:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "for i in range(CONFIG['num_classes']):\n",
    "    print(f\"\\n  Class {i} ({LABEL_MAP[i]}):\")\n",
    "    print(f\"    Precision: {val_results[f'precision_class_{i}']:.4f}\")\n",
    "    print(f\"    Recall: {val_results[f'recall_class_{i}']:.4f}\")\n",
    "    print(f\"    F1-Score: {val_results[f'f1_class_{i}']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed classification report\n",
    "labels_list = [LABEL_MAP[i] for i in range(CONFIG['num_classes'])]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    val_results['labels'], \n",
    "    val_results['predictions'],\n",
    "    target_names=labels_list,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix (counts)\n",
    "fig1 = plot_confusion_matrix(\n",
    "    val_results['labels'],\n",
    "    val_results['predictions'],\n",
    "    labels_list,\n",
    "    'Confusion Matrix - Validation Set'\n",
    ")\n",
    "plt.show()\n",
    "fig1.savefig(f\"{CONFIG['save_dir']}/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Plot normalized confusion matrix (percentages)\n",
    "fig2 = plot_normalized_confusion_matrix(\n",
    "    val_results['labels'],\n",
    "    val_results['predictions'],\n",
    "    labels_list,\n",
    "    'Normalized Confusion Matrix - Validation Set'\n",
    ")\n",
    "plt.show()\n",
    "fig2.savefig(f\"{CONFIG['save_dir']}/confusion_matrix_normalized.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"\\nüíæ Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Per-Class Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-class metrics\n",
    "fig = plot_metrics_per_class(val_results, labels_list)\n",
    "plt.show()\n",
    "fig.savefig(f\"{CONFIG['save_dir']}/metrics_per_class.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "print(\"\\nüíæ Per-class metrics plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module1Predictor:\n",
    "    \"\"\"Inference class for Module 1\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        tokenizer_name: str = \"vinai/phobert-base-v2\",\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = PhoBERTClassifier(tokenizer_name, num_classes=4)\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Severity mapping\n",
    "        self.severity_map = LABEL_MAP\n",
    "    \n",
    "    def predict(self, text: str, max_length: int = 128) -> Dict:\n",
    "        \"\"\"\n",
    "        Predict severity for a single text\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with prediction results\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids, attention_mask)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            pred_class = torch.argmax(probs, dim=1).item()\n",
    "            confidence = probs[0][pred_class].item()\n",
    "        \n",
    "        return {\n",
    "            'label': pred_class + 1,  # Convert back to 1-4 scale\n",
    "            'severity': self.severity_map[pred_class],\n",
    "            'confidence': confidence,\n",
    "            'probabilities': {\n",
    "                self.severity_map[i]: probs[0][i].item() \n",
    "                for i in range(len(self.severity_map))\n",
    "            },\n",
    "            'original_text': text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictor\n",
    "predictor = Module1Predictor(\n",
    "    model_path=f\"{CONFIG['save_dir']}/best_model.pt\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"M·∫Øt t√¥i h∆°i kh√¥ v√† ng·ª©a nh·∫π\",\n",
    "    \"M·∫Øt r·∫•t ƒëau v√† ƒë·ªè, nh√¨n m·ªù\",\n",
    "    \"C·∫£m gi√°c kh√¥ m·∫Øt m·ªôt ch√∫t th√¥i\",\n",
    "    \"M·∫Øt b√¨nh th∆∞·ªùng, kh√¥ng c√≥ tri·ªáu ch·ª©ng g√¨\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = predictor.predict(text)\n",
    "    \n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Predicted Severity: {result['severity']} (Label: {result['label']})\")\n",
    "    print(f\"   Confidence: {result['confidence']:.4f}\")\n",
    "    print(f\"   Probabilities:\")\n",
    "    for severity, prob in result['probabilities'].items():\n",
    "        print(f\"     {severity}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "results_summary = {\n",
    "    'model_name': CONFIG['model_name'],\n",
    "    'num_classes': CONFIG['num_classes'],\n",
    "    'training_config': CONFIG,\n",
    "    'best_val_f1': trainer.best_val_f1,\n",
    "    'final_metrics': {\n",
    "        'accuracy': val_results['accuracy'],\n",
    "        'precision_macro': val_results['precision_macro'],\n",
    "        'precision_weighted': val_results['precision_weighted'],\n",
    "        'recall_macro': val_results['recall_macro'],\n",
    "        'recall_weighted': val_results['recall_weighted'],\n",
    "        'f1_macro': val_results['f1_macro'],\n",
    "        'f1_weighted': val_results['f1_weighted']\n",
    "    },\n",
    "    'per_class_metrics': {\n",
    "        LABEL_MAP[i]: {\n",
    "            'precision': val_results[f'precision_class_{i}'],\n",
    "            'recall': val_results[f'recall_class_{i}'],\n",
    "            'f1': val_results[f'f1_class_{i}']\n",
    "        }\n",
    "        for i in range(CONFIG['num_classes'])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{CONFIG['save_dir']}/results_summary.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ All results saved to: {CONFIG['save_dir']}\")\n",
    "print(f\"\\nüìä Files saved:\")\n",
    "print(f\"  - best_model.pt\")\n",
    "print(f\"  - training_history.json\")\n",
    "print(f\"  - results_summary.json\")\n",
    "print(f\"  - training_history.png\")\n",
    "print(f\"  - confusion_matrix.png\")\n",
    "print(f\"  - confusion_matrix_normalized.png\")\n",
    "print(f\"  - metrics_per_class.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Summary\n",
    "\n",
    "### What this notebook does:\n",
    "\n",
    "1. **Data Loading**: Loads training and validation datasets for eye symptom classification\n",
    "2. **Data Preprocessing**: Tokenizes Vietnamese text using PhoBERT tokenizer\n",
    "3. **Model Architecture**: Uses PhoBERT with a custom classification head\n",
    "4. **Training**: Fine-tunes the model with AdamW optimizer and learning rate scheduling\n",
    "5. **Evaluation**: Comprehensive metrics including:\n",
    "   - Accuracy\n",
    "   - Precision (macro & weighted)\n",
    "   - Recall (macro & weighted)\n",
    "   - F1-Score (macro & weighted)\n",
    "   - Per-class metrics\n",
    "   - Confusion Matrix\n",
    "6. **Visualization**: Training curves, confusion matrices, and per-class performance\n",
    "7. **Inference**: Easy-to-use predictor class for making predictions on new text\n",
    "\n",
    "### Key Features:\n",
    "- Multi-class classification (4 severity levels)\n",
    "- Vietnamese language support via PhoBERT\n",
    "- Comprehensive evaluation metrics\n",
    "- Beautiful visualizations\n",
    "- Model checkpointing\n",
    "- Easy inference interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
